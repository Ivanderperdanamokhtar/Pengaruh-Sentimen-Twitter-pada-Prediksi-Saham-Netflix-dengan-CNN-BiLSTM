{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#VADER"
      ],
      "metadata": {
        "id": "sXaluV7_kdx_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQeUqlRg5uQ0",
        "outputId": "ecb8ded6-de01-4b6d-9aab-0de22ddb350e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m586.9/586.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.14.0\n"
          ]
        }
      ],
      "source": [
        "pip install emoji\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAZAGQJp5hac",
        "outputId": "98792304-ce00-4285-caf0-75f012571f76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                              Tweet  \\\n",
            "0  2022-07-11  Important since $nflx may start cracking down ...   \n",
            "1  2022-07-11  @QTRResearch $GOOGL technicals look good (brok...   \n",
            "2  2022-07-11  @MikeInTheClutch @joecarlsonshow Exactly. Ther...   \n",
            "3  2022-07-11  And that's how You negotiate an exclusivity de...   \n",
            "4  2022-07-11  Trending Stocks on #fintwit: \\n\\nSource: finap...   \n",
            "\n",
            "                                     Processed_Tweet  Negative  Neutral  \\\n",
            "0  import sinc nflx may start crack password shar...     0.000    0.804   \n",
            "1  googl technic look good broke day split cataly...     0.088    0.820   \n",
            "2  exactli may never way weigh babatrademark valu...     0.095    0.905   \n",
            "3  that negoti exclus dealthat nflx need key show...     0.000    0.804   \n",
            "4  trend stock fintwit sourc finapsel tsla twtr s...     0.000    1.000   \n",
            "\n",
            "   Positive  Compound Sentiment  \n",
            "0     0.196    0.2960   Neutral  \n",
            "1     0.091    0.0258   Neutral  \n",
            "2     0.000   -0.2732   Neutral  \n",
            "3     0.196    0.2960   Neutral  \n",
            "4     0.000    0.0000   Neutral  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Unduh resource yang dibutuhkan NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "nltk.download('punkt_tab') # Download the missing 'punkt_tab' resource\n",
        "\n",
        "# Muat data\n",
        "data = pd.read_excel('/content/Ntweets.xlsx')  # Sesuaikan path dengan lokasi file Anda\n",
        "data = data.rename(columns={'date': 'Date', 'renderedContent': 'Tweet'})\n",
        "\n",
        "# Konversi kolom 'Date' menjadi format tanggal\n",
        "data['Date'] = pd.to_datetime(data['Date']).dt.date\n",
        "\n",
        "# Fungsi untuk pra-pemrosesan tweet\n",
        "def preprocess_tweet(tweet):\n",
        "    new_tweet = tweet\n",
        "    new_tweet = re.sub(r'https?://[^ ]+', '', new_tweet)  # Hapus URL\n",
        "    new_tweet = re.sub(r'@[^ ]+', '', new_tweet)  # Hapus mentions\n",
        "    new_tweet = re.sub(r'#', '', new_tweet)  # Hapus simbol '#'\n",
        "    new_tweet = re.sub(r'([A-Za-z])\\1{2,}', r'\\1', new_tweet)  # Mengurangi pengulangan karakter\n",
        "    new_tweet = emoji.demojize(new_tweet)  # Ubah emoji menjadi teks\n",
        "    new_tweet = re.sub(r'[^A-Za-z ]', '', new_tweet)  # Hapus karakter non-alphabet\n",
        "    new_tweet = new_tweet.lower()  # Konversi ke huruf kecil\n",
        "\n",
        "    # Tokenisasi, hapus stopwords, dan stem\n",
        "    tokens = word_tokenize(new_tweet)\n",
        "    porter = PorterStemmer()\n",
        "    tokens = [porter.stem(token) for token in tokens if token not in stopwords.words('english')]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Terapkan pra-pemrosesan ke kolom 'Tweet'\n",
        "data['Processed_Tweet'] = data['Tweet'].apply(preprocess_tweet)\n",
        "\n",
        "# Inisialisasi Sentiment Intensity Analyzer\n",
        "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Analisis sentimen dan tambahkan kolom untuk skor sentimen\n",
        "data['Negative'] = 0.0\n",
        "data['Neutral'] = 0.0\n",
        "data['Positive'] = 0.0\n",
        "data['Compound'] = 0.0\n",
        "\n",
        "for indx, row in data.iterrows():\n",
        "    sentence_sentiment = sentiment_analyzer.polarity_scores(row['Processed_Tweet'])\n",
        "    data.at[indx, 'Negative'] = sentence_sentiment['neg']\n",
        "    data.at[indx, 'Neutral'] = sentence_sentiment['neu']\n",
        "    data.at[indx, 'Positive'] = sentence_sentiment['pos']\n",
        "    data.at[indx, 'Compound'] = sentence_sentiment['compound']\n",
        "\n",
        "# Salin dataframe dan tambahkan label sentimen\n",
        "labeled_data = data.copy()\n",
        "for indx, row in labeled_data.iterrows():\n",
        "    if row['Compound'] > 0.5:\n",
        "        labeled_data.at[indx, 'Sentiment'] = 'Positive'\n",
        "    elif row['Compound'] < -0.5:\n",
        "        labeled_data.at[indx, 'Sentiment'] = 'Negative'\n",
        "    else:\n",
        "        labeled_data.at[indx, 'Sentiment'] = 'Neutral'\n",
        "\n",
        "# Simpan data yang telah diproses ke file CSV\n",
        "labeled_data.to_csv('tweet.csv', index=False)\n",
        "\n",
        "# Tampilkan beberapa baris pertama dari hasil akhir\n",
        "print(labeled_data[['Date', 'Tweet', 'Processed_Tweet', 'Negative', 'Neutral', 'Positive', 'Compound', 'Sentiment']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simpan data yang telah diproses ke file CSV\n",
        "labeled_data.to_csv('tweet.csv', index=False)\n",
        "\n",
        "# Simpan data yang telah diproses ke file Excel (XLSX)\n",
        "labeled_data.to_excel('tweet.xlsx', index=False)\n",
        "\n",
        "# Tampilkan beberapa baris pertama dari hasil akhir\n",
        "print(labeled_data[['Date', 'Tweet', 'Processed_Tweet', 'Negative', 'Neutral', 'Positive', 'Compound', 'Sentiment']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prBBrEdzcEP0",
        "outputId": "4ee0ba6b-7892-423e-dc51-f90a4c1419aa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Date                                              Tweet  \\\n",
            "0  2022-07-11  Important since $nflx may start cracking down ...   \n",
            "1  2022-07-11  @QTRResearch $GOOGL technicals look good (brok...   \n",
            "2  2022-07-11  @MikeInTheClutch @joecarlsonshow Exactly. Ther...   \n",
            "3  2022-07-11  And that's how You negotiate an exclusivity de...   \n",
            "4  2022-07-11  Trending Stocks on #fintwit: \\n\\nSource: finap...   \n",
            "\n",
            "                                     Processed_Tweet  Negative  Neutral  \\\n",
            "0  import sinc nflx may start crack password shar...     0.000    0.804   \n",
            "1  googl technic look good broke day split cataly...     0.088    0.820   \n",
            "2  exactli may never way weigh babatrademark valu...     0.095    0.905   \n",
            "3  that negoti exclus dealthat nflx need key show...     0.000    0.804   \n",
            "4  trend stock fintwit sourc finapsel tsla twtr s...     0.000    1.000   \n",
            "\n",
            "   Positive  Compound Sentiment  \n",
            "0     0.196    0.2960   Neutral  \n",
            "1     0.091    0.0258   Neutral  \n",
            "2     0.000   -0.2732   Neutral  \n",
            "3     0.196    0.2960   Neutral  \n",
            "4     0.000    0.0000   Neutral  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/tweet.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'Date' column to datetime format for grouping purposes\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "# Calculating daily P_sum and P_mean\n",
        "# Here, we will consider 'Compound' as the sentiment score for each tweet.\n",
        "# Group by 'Date' to sum up compound scores and count tweets per day\n",
        "daily_sentiment = data.groupby('Date').agg(\n",
        "    P_sum=('Compound', 'sum'),        # Sum of Compound scores for each day\n",
        "    twt_count=('Compound', 'size')    # Count of tweets for each day\n",
        ")\n",
        "\n",
        "# Calculate P_mean\n",
        "daily_sentiment['P_mean'] = daily_sentiment['P_sum'] / daily_sentiment['twt_count']\n",
        "# Save the result to a new CSV file\n",
        "output_path = '/content/Olahan data Netflix.csv'\n",
        "daily_sentiment.to_csv(output_path, index=False) # Changed to_excel to to_csv\n",
        "\n",
        "print(f\"The daily sentiment data has been saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "8mNlGZdWvfOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8adc686-5c1b-481e-eb9a-6c1b7dc22726"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The daily sentiment data has been saved to /content/Olahan data Netflix.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}